# Progress Tracker

## Completed Phases

### Phase 0: Project Scaffolding (Done)
- **Step 0.1** — Directory structure, `pyproject.toml`, `requirements.txt`, `.env.example`, `.gitignore`
- **Step 0.2** — `src/config.py` (get_project_root, load_config, get_env, setup_logging) + `config/config.yaml` + `tests/test_config.py`

### Phase 2: LLM Provider Abstraction (Done)
- **Step 2.1** — `LLMProvider` ABC in `src/llm/base.py` with `complete()` and `complete_json()` abstract methods. Factory function `create_llm_provider()` in `src/llm/__init__.py`.
- **Step 2.2** — `OpenAIProvider` in `src/llm/openai_provider.py`. Uses `openai.AsyncOpenAI`, supports `response_format=json_object` for structured output.
- **Step 2.3** — `ClaudeProvider` in `src/llm/claude_provider.py`. Uses `anthropic.AsyncAnthropic`, auto-strips markdown code fences from JSON responses.
- **Step 2.4** — `ClaudeCodeProvider` in `src/llm/claude_code_provider.py`. Calls `claude` CLI via `asyncio.create_subprocess_exec`, zero API cost via subscription.
- **Tests** — 4 test files (`test_llm_base.py`, `test_llm_openai.py`, `test_llm_claude.py`, `test_llm_claude_code.py`). All tests pass. All mocked — no real API calls.

### Phase 3: ArXiv Fetcher (Done)
- **Step 3.1** — `ArxivFetcher` in `src/fetcher/arxiv_fetcher.py`. Fetches papers from configurable arXiv categories via the `arxiv` Python library. Python-side date filtering (arXiv API sorts but doesn't filter by date). Version suffix stripping from arxiv_id (e.g., `v2`). Cross-category deduplication. Uses `asyncio.to_thread` to avoid blocking the event loop.
- **Tests** — `tests/test_fetcher.py` with 10 tests covering: date filtering, cutoff_days parameter, cross-category deduplication, version stripping, field extraction, and edge cases. All mocked — no real arXiv API calls.

### Phase 4: Embedding System (Done)
- **Step 4.1** — `Embedder` class in `src/matcher/embedder.py`. Lazy-loads `sentence-transformers` model (`all-MiniLM-L6-v2`, 384 dims). Methods: `embed_text()` (single string → 1D array), `embed_texts()` (batch → 2D array), `serialize_embedding()` / `deserialize_embedding()` (numpy ↔ bytes for SQLite BLOBs), `compute_embeddings()` (batch-embeds paper abstracts via store), `compute_interest_embeddings()` (embeds interest text with optional description via store).
- **Step 4.2** — `find_similar()` method: matrix cosine similarity (`papers @ interests.T`), takes MAX score per paper across all interests, filters by configurable threshold, returns top-N sorted descending with `embedding_score` field.
- **Tests** — `tests/test_embedder.py` with 17 tests covering: embedding shape/normalization, serialize/deserialize round-trip, compute methods with mocked store, lazy model loading, `find_similar` (top-N selection, descending sort, threshold filtering, empty inputs, field preservation, MAX-across-interests). Tests use the real model (module-scoped fixture to avoid reloading).

### Phase 1: Database Layer (Done)
- **Step 1.1** — `PaperStore` class in `src/store/database.py`. Schema initialization for 5 tables (papers, interests, matches, summaries, daily_reports). WAL mode enabled, foreign keys enforced. Idempotent `_init_db()` with `CREATE TABLE IF NOT EXISTS`. Each method opens and closes its own connection via `_get_conn()` (row_factory = sqlite3.Row for dict-like access).
- **Step 1.2** — Paper CRUD: `save_papers` (INSERT OR IGNORE for deduplication, returns only newly inserted papers with assigned IDs), `get_paper_by_arxiv_id`, `get_papers_by_date`, `search_papers` (LIKE on title/abstract), `update_paper_embedding`, `get_papers_without_embeddings`, `get_papers_with_embeddings`, `get_papers_by_date_with_embeddings`. Authors and categories stored as JSON strings, deserialized on read via `_row_to_paper()`.
- **Step 1.3** — Interest CRUD: `save_interest`, `get_all_interests`, `get_interest_by_id`, `update_interest` (partial updates — only non-None fields), `delete_interest`, `update_interest_embedding`, `get_interests_with_embeddings`.
- **Step 1.4** — Match CRUD: `save_match`, `get_matches_by_date` (JOIN with papers table, ORDER BY llm_score DESC with NULLS LAST, then embedding_score DESC). Summary CRUD: `save_summary`, `get_summary` (by paper_id + summary_type). Report CRUD: `save_report`, `get_report_by_date`, `get_all_report_dates` (sorted descending).
- **Tests** — `tests/test_store.py` with 30 tests across 6 test classes: TestSchemaInit (3 tests), TestPaperCRUD (14 tests), TestInterestCRUD (10 tests), TestMatchCRUD (3 tests), TestSummaryCRUD (3 tests), TestReportCRUD (4 tests). All use `tmp_path` fixture for isolated temp DBs. No mocks needed — tests the real SQLite layer.

### Phase 6: LLM Re-ranker (Done)
- **Step 6.1** — `LLMRanker` class in `src/matcher/ranker.py`. Concurrent LLM re-ranking of embedding-filtered candidates. Uses `asyncio.gather` with `asyncio.Semaphore(max_concurrent)` to limit parallel LLM calls (default 5). Each paper scored by the LLM on a 1-10 scale with a reason. Graceful error handling: scoring failures return `llm_score=0` without crashing the batch. `_format_interests()` builds a readable text block from interest dicts for the LLM prompt.
- **Tests** — `tests/test_ranker.py` with 13 tests across 4 test classes: TestLLMRankerBasic (7 tests — top_k truncation, LLM field presence, original field preservation, call count per candidate, top_k override, descending sort, empty candidates), TestLLMRankerFailure (2 tests — invalid JSON returns score 0, partial failure doesn't crash), TestLLMRankerConcurrency (2 tests — semaphore respects max_concurrent, default parallelism works), TestFormatInterests (3 tests — keywords with description, mixed types, empty list). All mocked — no real LLM API calls.

### Phase 7: Report Generator (Done)
- **Step 7.1** — `ReportGenerator.generate_general()` in `src/report/generator.py`. Produces a Markdown general report with three sections: (1) "Today's Overview" — total paper count + per-category breakdown computed in pure Python via `collections.Counter` on primary category, (2) "Trending Topics" — LLM identifies 3-5 emerging topics from paper titles, (3) "Highlight Papers" — LLM selects 3-5 noteworthy papers with reasons. Graceful error handling: LLM failures produce an error message instead of crashing.
- **Step 7.2** — `ReportGenerator.generate_specific()`. Formats pre-scored data from the ranker into Markdown — no LLM calls. Numbered list with `llm_score/10` and `llm_reason` for each paper, followed by a "Related Papers" section with authors, categories, abstract preview (first 200 chars), and arXiv links. Handles empty results, string-type authors/categories gracefully.
- **Tests** — `tests/test_report_generator.py` with 22 tests across 3 test classes: TestGenerateGeneral (9 tests — header with date, total count, category breakdown, trending/highlight sections, LLM call count, empty papers, section headers), TestGenerateSpecific (9 tests — header, paper titles, LLM scores, reasons, related papers, arXiv links, no LLM calls, empty results, authors/categories in related), TestEdgeCases (4 tests — string authors, string categories, LLM failure handling, single-category papers). All mocked — no real LLM API calls.

### Phase 8: Email Sender (Done)
- **Step 8.1** — `EmailSender` class in `src/email/sender.py`. SMTP email delivery with Markdown → HTML → CSS-inline pipeline. Reads SMTP config (host, port) from `config["email"]["smtp"]` and credentials from environment variables. `render_markdown_to_html()` converts Markdown to HTML via the `markdown` library (with `tables` + `fenced_code` extensions), wraps in an HTML template with a `<style>` block (body font, headings, tables, links, code), then inlines CSS via `premailer.transform()`. `send()` combines general + specific reports, renders to HTML, builds a `MIMEMultipart` message, and sends via `smtplib.SMTP` (starttls + login + send_message) wrapped in `asyncio.to_thread()` to avoid blocking the event loop.
- **Tests** — `tests/test_email_sender.py` with 22 tests across 4 test classes: TestInit (6 tests — SMTP settings, env credentials, addresses, subject prefix, custom prefix, multiple recipients), TestRenderMarkdownToHtml (7 tests — heading, bold, inline CSS, table, link, horizontal rule, bullet list), TestBuildEmail (6 tests — MIME type, subject/from/to headers, multiple recipients, HTML payload), TestSend (5 tests — SMTP starttls/login/send_message calls, correct subject, report content merging, no real email sent, custom host/port). All mocked — no real SMTP connections.

### Phase 9: Paper Summarizer (Done)
- **Step 9.1** — `PaperSummarizer` class in `src/summarizer/paper_summarizer.py`. Fetches full paper text from ar5iv HTML pages via `requests` + `BeautifulSoup` with `lxml` parser. Looks for `<article>` tag first, falls back to `ltx_document` → `ltx_page_main` → body. Extracts text from `<p>`, `<h2>`, `<h3>` tags, truncates to 15,000 characters for LLM context safety. Two summarization modes: "brief" (1-2 paragraphs on contributions + methodology) and "detailed" (structured: Motivation, Method, Experiments, Conclusions, Limitations). Cache-first: checks `store.get_summary()` before calling LLM. Falls back to abstract if ar5iv fetch fails. Caches results via `store.save_summary()` with LLM provider name. Includes `_get_paper_by_id()` helper for integer-id lookup (PaperStore only has `get_paper_by_arxiv_id`).
- **Tests** — `tests/test_summarizer.py` with 19 tests across 3 test classes: TestFetchPaperText (8 tests — `<article>` extraction, heading extraction, `ltx_document`/`ltx_page_main` fallbacks, HTTP error, connection error, 15K truncation, empty tag skipping), TestSummarize (9 tests — cache hit skips LLM, brief/detailed prompt construction, cache persistence with provider name, abstract fallback on fetch failure, nonexistent paper ValueError, title in prompt, system prompt, separate brief/detailed caching), TestGetPaperById (2 tests — found/not found). All use real `PaperStore` with temp DBs and concrete `MockLLMProvider` — no real API calls or HTTP requests.

### Phase 5: Interest Manager (Done)
- **Step 5.1** — `InterestManager` class in `src/interest/manager.py`. CRUD operations for three interest types (keyword, paper, reference_paper) with automatic embedding computation on add/update. Three-tier auto-fetch for paper abstracts: (1) check DB via `store.get_paper_by_arxiv_id`, (2) fetch from arXiv API via `arxiv.Search(id_list=...)`, (3) fall back to using arxiv_id as text with a warning. `recompute_all_embeddings()` for model changes. All methods delegate storage to `PaperStore` and embedding to `Embedder`.
- **Tests** — `tests/test_interest_manager.py` with 16 tests across 6 test classes: TestAddKeyword (4 tests — returns int ID, creates interest in store, computes 384-dim embedding, description support), TestAddPaper (5 tests — with description, computes embedding, auto-fetch from DB, auto-fetch from arXiv via mock, fallback to ID), TestAddReferencePaper (2 tests — with description, auto-fetch from DB), TestUpdateAndRemove (2 tests — update changes value+embedding, remove deletes), TestRecomputeAll (1 test), TestFetchAbstractFromArxiv (3 tests — success, no results, exception). Uses real `PaperStore` + real `Embedder` (module-scoped fixture); arXiv API calls mocked.

### Phase 10: Pipeline Orchestrator (Done)
- **Step 10.1** — `DailyPipeline` class in `src/pipeline.py`. Wires all 8 components together: `PaperStore`, `ArxivFetcher`, `Embedder`, `LLMProvider` (via factory), `LLMRanker`, `InterestManager`, `ReportGenerator`, `EmailSender`. The `run()` method executes the 12-step daily pipeline: fetch → save → embed → check interests → match today's papers → re-rank → save matches → generate general report → generate specific report → send email (if enabled) → save report → return summary dict. Two code paths: (a) with interests — full matching + ranking + both reports, (b) without interests — skip matching, generate general report only. Email failures are caught and logged without crashing the pipeline.
- **Tests** — `tests/test_pipeline.py` with 8 tests across 2 test classes: TestDailyPipelineFullRun (7 tests — full happy path verifying all component calls, no-interests skips matching, email disabled skips sending, email failure doesn't crash, save_match called per ranked paper, save_report called once with correct args, no-interests still saves general report), TestDailyPipelineInit (1 test — verifies all components instantiated). All components mocked — no real API calls, no real DB (except PaperStore which uses tmp_path).

### Phase 11: Scheduler and CLI Entry Points (Done)
- **Step 11.1** — `PipelineScheduler` class in `src/scheduler/scheduler.py`. Wraps APScheduler's `BlockingScheduler` with `CronTrigger`. Parses standard 5-field cron strings from `config["scheduler"]["cron"]`. `start()` adds the pipeline job and blocks. `_run_pipeline()` creates a `DailyPipeline` instance and runs it via `asyncio.run()`. Includes logging for scheduler start and pipeline completion.
- **Step 11.2** — CLI entry point in `src/main.py` and CI/CD script in `scripts/run_pipeline.py`. `src/main.py` uses `argparse` with `--mode` (`scheduler`|`run`, default `run`) and `--config` (optional path override). Calls `setup_logging()` and `load_config()` at startup, then dispatches to `PipelineScheduler.start()` or `asyncio.run(pipeline.run())`. `scripts/run_pipeline.py` is a standalone entry point for GitHub Actions that adds project root to `sys.path`, loads config, runs the pipeline, and logs a warning if no new papers were fetched.
- **Tests** — `tests/test_scheduler.py` with 9 tests across 3 test classes: TestPipelineSchedulerInit (2 tests — config storage, BlockingScheduler creation), TestPipelineSchedulerStart (5 tests — cron trigger creation, field parsing for various cron expressions, scheduler.start called), TestPipelineSchedulerRunPipeline (2 tests — pipeline creation+execution, config passing). `tests/test_main.py` with 7 tests across 3 test classes: TestMainRunMode (3 tests — pipeline execution, custom config path, default mode is run), TestMainSchedulerMode (2 tests — scheduler start, config passing), TestRunPipelineScript (2 tests — execution, no-papers warning). All mocked — no real scheduling, no real pipeline execution.

## Next Up

### Phase 12: Streamlit GUI
- Step 12.1: Main app entry with navigation (`gui/app.py`)
- Step 12.2: Dashboard page
- Step 12.3: Papers page with browsing, search, and summarization
- Step 12.4: Interests management page
- Step 12.5: Reports page
- Step 12.6: Settings page
- Step 12.7: Automated GUI tests with Streamlit AppTest

## Notes for Future Developers
- Phase 2 was implemented before Phase 1 because it only depends on Phase 0 (no DB dependency).
- Phase 4 was implemented before Phase 1. The `compute_embeddings` and `compute_interest_embeddings` methods accept a `store` parameter (duck-typed), so they work without a concrete `PaperStore` — tests use `MagicMock`.
- Phase 1 was implemented 5th (after 0, 2, 3, 4) because earlier phases duck-typed the store dependency. Now that PaperStore exists, Phase 5 (InterestManager) can use the real store in its tests.
- All LLM providers use async interfaces. Tests mock the underlying SDK clients — no API keys needed to run tests.
- `ClaudeCodeProvider` sends the system message prepended to the user prompt via stdin (CLI doesn't have a separate system parameter).
- `ClaudeProvider` and `ClaudeCodeProvider` both strip markdown code fences (```json ... ```) from JSON responses since these models may wrap JSON in code blocks.
- The factory function uses lazy imports to avoid loading unused SDK dependencies.
- Embedder tests use a module-scoped fixture (`scope="module"`) so the ~80MB model is loaded only once across all test functions. The `find_similar` tests use synthetic normalized vectors via `_make_normalized_vector()` helper — no model loading needed for those.
- PaperSummarizer tests use real `PaperStore` (not mocks) because `_get_paper_by_id()` accesses `store._get_conn()` directly. A concrete `MockLLMProvider` class (with `call_count`, `last_prompt`, `last_system` tracking) is used instead of `MagicMock` for cleaner async behavior. HTTP requests are patched at the module level (`src.summarizer.paper_summarizer.requests.get`).
- PaperStore tests use `tmp_path` for isolated databases — no cleanup needed, no interference between tests. Each test class covers one CRUD domain (papers, interests, matches, summaries, reports).
- LLMRanker tests use concrete mock `LLMProvider` subclasses (not `MagicMock`) for cleaner async behavior. `MockLLMProviderConcurrency` uses an `asyncio.Lock` counter + `asyncio.sleep(0.05)` to verify the semaphore limits parallel execution. Tests use `max_concurrent=1` when deterministic ordering matters (e.g., the descending sort test with a `VaryingScoreLLM`).
- ReportGenerator tests also use concrete mock `LLMProvider` subclasses. The mock dispatches canned responses based on prompt keywords ("trending"/"emerging" vs "noteworthy"/"impactful"). `generate_specific` is tested to confirm it makes zero LLM calls — it only formats pre-scored data.
- `src/main.py` uses local imports inside `main()` (lazy imports for `load_config`, `setup_logging`, `DailyPipeline`, `PipelineScheduler`). Tests must patch at the definition site (`src.config.setup_logging`, `src.config.load_config`, `src.pipeline.DailyPipeline`, `src.scheduler.scheduler.PipelineScheduler`) rather than at `src.main.*`, because local imports don't create module-level attributes that `unittest.mock.patch` can find.
- `scripts/run_pipeline.py` uses top-level imports, so its tests patch at `scripts.run_pipeline.*` directly.
